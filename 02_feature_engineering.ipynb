{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 02: Feature Engineering\n",
    "Building on our exploratory data analysis, we now transition into feature engineering to refine and enrich our dataset, thereby enhancing its predictive power for cyclist traffic in Paris.\n",
    "\n",
    "In this notebook, we focus on:\n",
    "\n",
    "- **Refining Data Quality:** \n",
    "\n",
    "    Address any remaining data inconsistencies or missing values identified during EDA.\n",
    "\n",
    "- **Temporal Feature Extraction:** \n",
    "\n",
    "    Leverage date and time data to create meaningful features like day of the week, hour, and season.\n",
    "\n",
    "- **Integrating External Data:** \n",
    "\n",
    "    Enrich the dataset with external factors like weather conditions, potentially influential in predicting cyclist traffic.\n",
    "\n",
    "- **Transforming and Creating Features:** \n",
    "\n",
    "    Apply techniques like one-hot encoding and normalization, and develop new features informed by EDA insights.\n",
    "\n",
    "- **Strategic Feature Selection:** \n",
    "\n",
    "    Employ methods to select impactful features, aiming to reduce complexity while maintaining model efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "#pip install holidays\n",
    "#pip install vacances-scolaires-france\n",
    "#pip install lockdowndates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "import holidays\n",
    "from meteostat import Point, Hourly\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from lockdowndates.core import LockdownDates\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "bike_df_train = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "bike_df_test = pd.read_parquet(Path(\"data\") / \"test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**:\n",
    "In this phase, we extract and refine various features from our dataset to enhance the predictive modeling of bike traffic in Paris. The process involves:\n",
    "\n",
    "- **Date and Time Extraction**: \n",
    "\n",
    "    We dissect the `date` column to extract key temporal components - `year`, `month`, `day`, and `hour`. This breakdown aids in understanding traffic patterns across different time features.\n",
    "\n",
    "- **Seasonality Identification**: \n",
    "\n",
    "    By mapping month to `season` and identifying `weekdays` and French public `holidays`, we capture seasonal and weekly traffic trends. Additionally, we integrate `school holiday` data specific to Paris (Zone C) to account for variations during school breaks.\n",
    "\n",
    "- **Weather Data Integration**: \n",
    "\n",
    "    Utilizing the `meteostat` library, we fetch and clean `weather` data for distinct geographical locations in our dataset. By using the geolocalization coordinates (`latitude` and `longitude`) of each site, we can extract the hourly weather at each of the locations. This step ensures each entry in our dataset is enriched with relevant and timely weather information, which is vital for analyzing its impact on bike traffic.\n",
    "\n",
    "These enhancements in feature extraction aim to provide a comprehensive and nuanced view of the factors affecting bike traffic in Paris, setting the stage for more accurate and insightful modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_df_train['date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently only have a column `date` of type `<M8[us]` which is a datetime with microsecond precision. We can take advantage of this date datatype to create more explicit columns of each information regarding a date (e.g. `year`, `month`, `day`, `hour`).\n",
    "\n",
    "We will go even further and add information regarding whether the day is a `working day`, a `holiday` and also consider the `seasons` for a more general analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    \"\"\"\n",
    "    Map a given month to a corresponding season.\n",
    "\n",
    "    Parameters:\n",
    "    - month (int): Numeric representation of the month (1 to 12).\n",
    "\n",
    "    Returns:\n",
    "    - str: The season associated with the input month. Possible values are 'Spring', 'Summer', 'Fall', or 'Winter'.\n",
    "\n",
    "    Example:\n",
    "    >>> get_season(7)\n",
    "    'Summer'\n",
    "\n",
    "    Note:\n",
    "    The function considers the following month ranges for each season:\n",
    "    - Spring: March (3) to May (5)\n",
    "    - Summer: June (6) to August (8)\n",
    "    - Fall: September (9) to November (11)\n",
    "    - Winter: December (12), January (1), and February (2)\n",
    "    \"\"\"\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter' # December, January, February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SchoolHolidayDates object\n",
    "school_holidays = SchoolHolidayDates()\n",
    "\n",
    "#Check if a given date is a school holiday\n",
    "def is_school_holiday(datetime_obj):\n",
    "    \"\"\"\n",
    "    Check if a given date, extracted from a datetime object, corresponds to a school holiday in Zone C (Paris).\n",
    "\n",
    "    Parameters:\n",
    "    - datetime_obj (datetime.datetime): Input datetime object.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the date corresponds to a school holiday, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extracting just the date part from the datetime object\n",
    "    date_obj = datetime_obj.date()\n",
    "    return school_holidays.is_holiday_for_zone(date_obj, 'C')  # Paris is Zone C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dates(df):\n",
    "    \"\"\"\n",
    "    Encode temporal features from a given DataFrame containing a 'date' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with a 'date' column of datetime type.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with additional columns encoding temporal features, including:\n",
    "        - 'year': Year extracted from the 'date' column.\n",
    "        - 'month': Month extracted from the 'date' column.\n",
    "        - 'day': Day of the month extracted from the 'date' column.\n",
    "        - 'hour': Hour of the day extracted from the 'date' column.\n",
    "        - 'weekday': Numeric representation of the day of the week (0 for Monday, 6 for Sunday).\n",
    "        - 'season': Season associated with the 'month' column using the get_season function.\n",
    "        - 'working_day': Binary indicator for working days (1 for weekdays not in French holidays, 0 otherwise).\n",
    "        - 'school_holiday': Binary indicator for school holidays using the is_school_holiday function.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the input DataFrame does not have a 'date' column or if the 'date' column is not of datetime type.\n",
    "    \"\"\"\n",
    "    if 'date' not in df.columns or not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        raise ValueError(\"DataFrame must have a 'date' column of datetime type.\")\n",
    "\n",
    "    X = df.copy()\n",
    "\n",
    "    # Extracting date components\n",
    "    X['year'] = X['date'].dt.year\n",
    "    X['month'] = X['date'].dt.month\n",
    "    X['day'] = X['date'].dt.day\n",
    "    X['hour'] = X['date'].dt.hour\n",
    "    X['weekday'] = X['date'].dt.weekday\n",
    "\n",
    "    # Adding season based on month\n",
    "    X['season'] = X['month'].apply(get_season)\n",
    "\n",
    "    # Identifying working days (weekdays not in French holidays)\n",
    "    fr_holidays = holidays.France()\n",
    "    X['working_day'] = ((X['date'].dt.weekday < 5) & ~X['date'].dt.date.isin(fr_holidays)).astype(int)\n",
    "\n",
    "    # Identifying school holidays\n",
    "    X['school_holiday'] = X['date'].apply(is_school_holiday).astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 247.72 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season</th>\n",
       "      <th>working_day</th>\n",
       "      <th>school_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48321</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 02:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48324</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-01 03:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48327</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 04:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48330</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-09-01 15:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-09-01 18:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>2.375429</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                counter_id              counter_name    site_id  \\\n",
       "48321  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48324  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48327  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48330  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "48333  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "\n",
       "                  site_name  bike_count                date  \\\n",
       "48321  28 boulevard Diderot         0.0 2020-09-01 02:00:00   \n",
       "48324  28 boulevard Diderot         1.0 2020-09-01 03:00:00   \n",
       "48327  28 boulevard Diderot         0.0 2020-09-01 04:00:00   \n",
       "48330  28 boulevard Diderot         4.0 2020-09-01 15:00:00   \n",
       "48333  28 boulevard Diderot         9.0 2020-09-01 18:00:00   \n",
       "\n",
       "      counter_installation_date         coordinates counter_technical_id  \\\n",
       "48321                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48324                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48327                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48330                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "48333                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "\n",
       "        latitude  longitude  log_bike_count  year  month  day  hour  weekday  \\\n",
       "48321  48.846028   2.375429        0.000000  2020      9    1     2        1   \n",
       "48324  48.846028   2.375429        0.693147  2020      9    1     3        1   \n",
       "48327  48.846028   2.375429        0.000000  2020      9    1     4        1   \n",
       "48330  48.846028   2.375429        1.609438  2020      9    1    15        1   \n",
       "48333  48.846028   2.375429        2.302585  2020      9    1    18        1   \n",
       "\n",
       "      season  working_day  school_holiday  \n",
       "48321   Fall            1               0  \n",
       "48324   Fall            1               0  \n",
       "48327   Fall            1               0  \n",
       "48330   Fall            1               0  \n",
       "48333   Fall            1               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "bike_df_train = encode_dates(bike_df_train)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the runtime\n",
    "runtime = round(end_time - start_time, 2)\n",
    "\n",
    "# Print the runtime\n",
    "print(f\"Runtime: {runtime} seconds\")\n",
    "bike_df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `weather` can have a great impact on bike traffic so we will also add weather information from `meteostat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(X):\n",
    "    \"\"\"\n",
    "    Retrieve hourly weather data for unique geographical locations and combine it into a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): Input DataFrame with columns 'date', 'latitude', and 'longitude' (among others).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Add new features on weather data for all unique locations, including columns:\n",
    "        - 'date': Date and time of the weather observation.\n",
    "        - 'temperature': Hourly temperature data.\n",
    "        - 'humidity': Hourly humidity data.\n",
    "        - 'precipitation': Hourly precipitation data.\n",
    "        - 'latitude': Constant latitude value for the location.\n",
    "        - 'longitude': Constant longitude value for the location.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the input DataFrame does not contain the required columns.\n",
    "    \"\"\"\n",
    "    if not all(col in X.columns for col in ['date', 'latitude', 'longitude']):\n",
    "        raise ValueError(\"DataFrame must have 'date', 'latitude', and 'longitude' columns.\")\n",
    "\n",
    "    unique_locations = X[['latitude', 'longitude']].drop_duplicates()\n",
    "    weather_data_list = []\n",
    "\n",
    "    for lat, lon in unique_locations.itertuples(index=False):\n",
    "        point = Point(lat, lon)\n",
    "        location_data = Hourly(point, X['date'].min(), X['date'].max()).fetch()\n",
    "        location_data.reset_index(inplace=True)  # Reset index to make 'date' a column\n",
    "        location_data.rename(columns={'time': 'date'}, inplace=True)  # Rename 'time' column to 'date'\n",
    "        location_data['latitude'], location_data['longitude'] = lat, lon\n",
    "        weather_data_list.append(location_data)\n",
    "\n",
    "    weather_data = pd.concat(weather_data_list, ignore_index=True)\n",
    "\n",
    "    # Ensure 'date' column is in the right format if it's not already\n",
    "    if weather_data['date'].dtype != 'datetime64[ns]':\n",
    "        weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "    # Merge data\n",
    "    merged_data = pd.merge(X, weather_data, on=['date', 'latitude', 'longitude'], how='left')\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 7.93 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 02:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1019.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-01 03:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1019.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 04:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-09-01 15:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1017.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard Diderot E-O</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard Diderot</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-09-01 18:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>Y2H15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            counter_id              counter_name    site_id  \\\n",
       "0  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "1  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "2  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "3  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "4  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
       "\n",
       "              site_name  bike_count                date  \\\n",
       "0  28 boulevard Diderot         0.0 2020-09-01 02:00:00   \n",
       "1  28 boulevard Diderot         1.0 2020-09-01 03:00:00   \n",
       "2  28 boulevard Diderot         0.0 2020-09-01 04:00:00   \n",
       "3  28 boulevard Diderot         4.0 2020-09-01 15:00:00   \n",
       "4  28 boulevard Diderot         9.0 2020-09-01 18:00:00   \n",
       "\n",
       "  counter_installation_date         coordinates counter_technical_id  \\\n",
       "0                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "1                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "2                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "3                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "4                2013-01-18  48.846028,2.375429          Y2H15027244   \n",
       "\n",
       "    latitude  ...  dwpt  rhum  prcp  snow   wdir  wspd  wpgt    pres  tsun  \\\n",
       "0  48.846028  ...   9.4  79.0   0.0   NaN  290.0   3.6   9.3  1019.9   NaN   \n",
       "1  48.846028  ...   9.7  81.0   0.0   0.0  310.0   5.4   5.0  1019.8   NaN   \n",
       "2  48.846028  ...   9.6  83.0   0.0   NaN  300.0   5.4   7.4  1019.5   NaN   \n",
       "3  48.846028  ...   7.8  46.0   0.0   0.0   30.0  14.8  28.0  1017.5   NaN   \n",
       "4  48.846028  ...   7.0  46.0   NaN   0.0   40.0  11.2  24.0  1017.6   NaN   \n",
       "\n",
       "   coco  \n",
       "0   1.0  \n",
       "1   1.0  \n",
       "2   1.0  \n",
       "3   3.0  \n",
       "4   4.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "bike_df_train = get_weather_data(bike_df_train)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the runtime\n",
    "runtime = round(end_time - start_time, 2)\n",
    "\n",
    "# Print the runtime\n",
    "print(f\"Runtime: {runtime} seconds\")\n",
    "bike_df_train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Data Cleaning**:\n",
    "The final data cleaning stage is critical in preparing our dataset for accurate and effective modeling. After incorporating external data sources, we now ensure the dataset's cleanliness and readiness for analysis. This involves:\n",
    "\n",
    "- **Handling Missing Values**: \n",
    "\n",
    "    Applying tailored imputation strategies (zero-fill for precipitation, mean-fill for temperature, humidity, wind speed, and KNN imputation for snowfall) to maintain data integrity\n",
    "\n",
    "- **Removing Duplicates**: \n",
    "\n",
    "    Eliminating duplicate entries to prevent skewed analysis\n",
    "\n",
    "- **Data Type Conversion**: \n",
    "\n",
    "    Converting the 'date' column to datetime format and setting it as the index for easier manipulation and analysis\n",
    "\n",
    "- **Dropping Redundant Columns**: \n",
    "\n",
    "    Removing columns that don't contribute to the predictive power of our model, like counter names, raw bike counts, and certain temporal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prcp     22428\n",
       "snow    341809\n",
       "tsun    496771\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "missing_values = bike_df_train.isna().sum()\n",
    "\n",
    "# Show only variables with null values greater than 1\n",
    "missing_values_greater_than_1 = missing_values[missing_values > 1]\n",
    "missing_values_greater_than_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are only missing values regarding the weather data we used that's why it's important to double check after our initial EDA.\n",
    "\n",
    "We cannot just remove rows that have missing values because we would lose a lot of information. Moreover, it is reasonable to assume that the amount of rain (`prcp`), snow (`snow`), and sunshine (`tsun`) do affect the number of bikes. Therefore, we need to consider what to do with all these missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways we could handle these missing values.\n",
    "\n",
    "#### 1. Replace missing values by 0\n",
    "\n",
    "This strategy is suitable when missing values are interpreted as zero or when you believe that the missing values occur randomly or don't carry significant information. When looking at the amount of rain, snow or sunshine, it makes sense to treat missing values as 0 (as if we don't have any information at those times). Be cautious when applying this strategy, especially if missing values are not distributed randomly. In cases where missing values represent a pattern or specific conditions, replacing them with zero may introduce bias. We would need to see if the missing values are scattered randomly in the dataset or if there are several rows with missing values.\n",
    "\n",
    "For example, it would be unreasonable to set the amount of rain to be 0 for the whole month of December.\n",
    "\n",
    "#### 2. Replace missing values by the mean / median\n",
    "\n",
    "Another strategy is to replace the amounts by either the mean or the median of the distribution. However, a mean value should be used on variable's with a \"normal\" distribution (no strong skewness, outliers). <br> A median value should be used when the distribution is skewed with many outliers. As in the case of strategy 1, this strategy assumes that the missing values are missing completely at random.\n",
    "\n",
    "#### 3. Replace missing values by the most frequent value:\n",
    "\n",
    "This approach is more suitable for categorical variables where the most frequent category can reasonably stand in for the missing values. This strategy may be less appropriate for continuous variables.\n",
    "\n",
    "#### 4. Use Machine Learning techniques to predict the missing values\n",
    "\n",
    "We can also use techniques such as KNN Imputer, for imputing missing values in a dataset. Instead of using a single value (like mean, median, or zero), KNN imputation estimates missing values by considering the values of their k-nearest neighbors. However, it adds a lot more computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `prcp`, setting missing values to 0 seems reasonable since we assume they indicate no precipitation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_train['prcp'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `tsun`, we believe the amount of sunshine does not influence the number of bikes in Paris. Indeed, whether it is cloudy or sunny, as long as it is not raining or snowing, regular bikers will still take the bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df_train.drop(columns=\"tsun\", inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `snow`, let's investigate the number of missing values in winter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries during Winter: 120518\n"
     ]
    }
   ],
   "source": [
    "winter_data = bike_df_train.query('season == \"Winter\"')\n",
    "\n",
    "# Check how many entries are there for the Winter season\n",
    "num_winter_entries = winter_data.shape[0]\n",
    "\n",
    "# Output the number of Winter entries\n",
    "print(f\"Number of entries during Winter: {num_winter_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snow    81858\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing amount of rain and snow\n",
    "winter_data[['snow']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that in winter, the weather is more or less stable and that we can estimate the missing amount of snow using `KNNImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time, we already ran the computation and saved it in a new file called bike_df_imputed\n",
    "knn_flag = False\n",
    "\n",
    "if(knn_flag):\n",
    "\n",
    "    # Apply imputation - Note: This might be computationally expensive for large datasets\n",
    "    bike_df_train[['snow']] = KNNImputer().fit_transform(bike_df_train[['snow']].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # Save the DataFrame with the imputed values in the 'data' directory\n",
    "    bike_df_train.to_parquet('data/bike_df_imputed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'snow' values before imputation: 341809\n",
      "Missing 'snow' values after imputation: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Imputation</th>\n",
       "      <th>After Imputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>155018.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.108375</td>\n",
       "      <td>0.108375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.492652</td>\n",
       "      <td>0.833769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Before Imputation  After Imputation\n",
       "count      155018.000000     496827.000000\n",
       "mean            0.108375          0.108375\n",
       "std             1.492652          0.833769\n",
       "min             0.000000          0.000000\n",
       "25%             0.000000          0.000000\n",
       "50%             0.000000          0.108375\n",
       "75%             0.000000          0.108375\n",
       "max            40.000000         40.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset after imputation\n",
    "bike_df_imputed = pd.read_parquet('data/bike_df_imputed.parquet')\n",
    "\n",
    "# Check the number of missing values for 'snow' before and after imputation\n",
    "missing_values_before = bike_df_train['snow'].isnull().sum()\n",
    "missing_values_after = bike_df_imputed['snow'].isnull().sum()\n",
    "\n",
    "# Calculate the summary statistics for 'snow' before and after imputation\n",
    "summary_before = bike_df_train['snow'].describe()\n",
    "summary_after = bike_df_imputed['snow'].describe()\n",
    "\n",
    "# Compare the distributions of 'snow' before and after imputation\n",
    "comparison_summary = pd.DataFrame({'Before Imputation': summary_before, 'After Imputation': summary_after})\n",
    "\n",
    "# Display the results\n",
    "print(f\"Missing 'snow' values before imputation: {missing_values_before}\")\n",
    "print(f\"Missing 'snow' values after imputation: {missing_values_after}\")\n",
    "comparison_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN imputation effectively addressed all missing values in the 'snow' column, reducing them from 341,809 to 0. Post-imputation, the distribution of 'snow' shows a lower standard deviation, indicating reduced variability, while the mean value remains almost unchanged. This suggests a smoothing effect on the data, with potential impacts on model performance that should be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "No duplicates found.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>log_bike_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.968270e+05</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "      <td>496827.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.053450e+08</td>\n",
       "      <td>60.191475</td>\n",
       "      <td>48.854343</td>\n",
       "      <td>2.345479</td>\n",
       "      <td>3.079917</td>\n",
       "      <td>2020.679846</td>\n",
       "      <td>6.556904</td>\n",
       "      <td>15.458226</td>\n",
       "      <td>11.502730</td>\n",
       "      <td>2.992172</td>\n",
       "      <td>...</td>\n",
       "      <td>12.917013</td>\n",
       "      <td>7.091982</td>\n",
       "      <td>70.013957</td>\n",
       "      <td>0.088281</td>\n",
       "      <td>0.108375</td>\n",
       "      <td>185.039364</td>\n",
       "      <td>11.321216</td>\n",
       "      <td>24.229571</td>\n",
       "      <td>1016.412786</td>\n",
       "      <td>4.032233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.210346e+07</td>\n",
       "      <td>87.590566</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.038026</td>\n",
       "      <td>1.659899</td>\n",
       "      <td>0.466536</td>\n",
       "      <td>3.423834</td>\n",
       "      <td>8.851485</td>\n",
       "      <td>6.920936</td>\n",
       "      <td>1.995015</td>\n",
       "      <td>...</td>\n",
       "      <td>6.873227</td>\n",
       "      <td>5.914273</td>\n",
       "      <td>16.055120</td>\n",
       "      <td>0.441823</td>\n",
       "      <td>0.833769</td>\n",
       "      <td>102.795006</td>\n",
       "      <td>5.112426</td>\n",
       "      <td>11.135895</td>\n",
       "      <td>9.478988</td>\n",
       "      <td>3.597789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000070e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.826360</td>\n",
       "      <td>2.265420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.400000</td>\n",
       "      <td>-17.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>972.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000475e+08</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.314440</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>1011.300000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000562e+08</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>48.852090</td>\n",
       "      <td>2.353870</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108375</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>1017.900000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000563e+08</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>48.864610</td>\n",
       "      <td>2.375870</td>\n",
       "      <td>4.382027</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108375</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1022.700000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000147e+08</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>48.891720</td>\n",
       "      <td>2.409690</td>\n",
       "      <td>7.172425</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>34.100000</td>\n",
       "      <td>20.700000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1039.500000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site_id     bike_count       latitude      longitude  \\\n",
       "count  4.968270e+05  496827.000000  496827.000000  496827.000000   \n",
       "mean   1.053450e+08      60.191475      48.854343       2.345479   \n",
       "std    3.210346e+07      87.590566       0.018554       0.038026   \n",
       "min    1.000070e+08       0.000000      48.826360       2.265420   \n",
       "25%    1.000475e+08       5.000000      48.840801       2.314440   \n",
       "50%    1.000562e+08      29.000000      48.852090       2.353870   \n",
       "75%    1.000563e+08      79.000000      48.864610       2.375870   \n",
       "max    3.000147e+08    1302.000000      48.891720       2.409690   \n",
       "\n",
       "       log_bike_count           year          month            day  \\\n",
       "count   496827.000000  496827.000000  496827.000000  496827.000000   \n",
       "mean         3.079917    2020.679846       6.556904      15.458226   \n",
       "std          1.659899       0.466536       3.423834       8.851485   \n",
       "min          0.000000    2020.000000       1.000000       1.000000   \n",
       "25%          1.791759    2020.000000       4.000000       8.000000   \n",
       "50%          3.401197    2021.000000       7.000000      15.000000   \n",
       "75%          4.382027    2021.000000       9.000000      23.000000   \n",
       "max          7.172425    2021.000000      12.000000      31.000000   \n",
       "\n",
       "                hour        weekday  ...           temp           dwpt  \\\n",
       "count  496827.000000  496827.000000  ...  496827.000000  496827.000000   \n",
       "mean       11.502730       2.992172  ...      12.917013       7.091982   \n",
       "std         6.920936       1.995015  ...       6.873227       5.914273   \n",
       "min         0.000000       0.000000  ...      -6.400000     -17.100000   \n",
       "25%         6.000000       1.000000  ...       7.900000       2.600000   \n",
       "50%        12.000000       3.000000  ...      12.500000       7.400000   \n",
       "75%        18.000000       5.000000  ...      17.800000      11.900000   \n",
       "max        23.000000       6.000000  ...      34.100000      20.700000   \n",
       "\n",
       "                rhum           prcp           snow           wdir  \\\n",
       "count  496827.000000  496827.000000  496827.000000  496827.000000   \n",
       "mean       70.013957       0.088281       0.108375     185.039364   \n",
       "std        16.055120       0.441823       0.833769     102.795006   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%        59.000000       0.000000       0.000000      80.000000   \n",
       "50%        73.000000       0.000000       0.108375     200.000000   \n",
       "75%        83.000000       0.000000       0.108375     260.000000   \n",
       "max       100.000000      16.000000      40.000000     360.000000   \n",
       "\n",
       "                wspd           wpgt           pres           coco  \n",
       "count  496827.000000  496827.000000  496827.000000  496827.000000  \n",
       "mean       11.321216      24.229571    1016.412786       4.032233  \n",
       "std         5.112426      11.135895       9.478988       3.597789  \n",
       "min         0.000000       1.800000     972.200000       1.000000  \n",
       "25%         7.600000      16.700000    1011.300000       2.000000  \n",
       "50%        11.200000      22.200000    1017.900000       3.000000  \n",
       "75%        14.800000      31.000000    1022.700000       4.000000  \n",
       "max        40.700000      94.000000    1039.500000      25.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = bike_df_imputed.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Remove duplicate rows if any\n",
    "if duplicate_rows > 0:\n",
    "    bike_df_imputed = bike_df_imputed.drop_duplicates()\n",
    "    print(\"Duplicates removed.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n",
    "\n",
    "# Check summary statistics to identify any anomalies\n",
    "bike_df_imputed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies in bike_count: 0\n",
      "Missing values in each column:\n",
      "counter_id                   0\n",
      "counter_name                 0\n",
      "site_id                      0\n",
      "site_name                    0\n",
      "bike_count                   0\n",
      "date                         0\n",
      "counter_installation_date    0\n",
      "coordinates                  0\n",
      "counter_technical_id         0\n",
      "latitude                     0\n",
      "longitude                    0\n",
      "log_bike_count               0\n",
      "year                         0\n",
      "month                        0\n",
      "day                          0\n",
      "hour                         0\n",
      "weekday                      0\n",
      "season                       0\n",
      "working_day                  0\n",
      "school_holiday               0\n",
      "temp                         0\n",
      "dwpt                         0\n",
      "rhum                         0\n",
      "prcp                         0\n",
      "snow                         0\n",
      "wdir                         0\n",
      "wspd                         0\n",
      "wpgt                         0\n",
      "pres                         0\n",
      "coco                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Validate key variables\n",
    "print(\"Anomalies in bike_count:\", bike_df_imputed[bike_df_imputed['bike_count'] < 0].shape[0])\n",
    "\n",
    "# Final check for missing values\n",
    "missing_values = bike_df_imputed.isna().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature Engineering\n",
    "\n",
    "In this section, we automate all of the feature engineering process for simplicity and avoid redundancy when performing feature engineering on our training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating new Features**\n",
    "In this stage, we focus on enhancing our dataset with creatively engineered features, tailored to capture the complex dynamics of bike traffic in Paris. We delve into:\n",
    "\n",
    "- **Weather Categorization**: \n",
    "\n",
    "    We create a 'weather' feature categorizing conditions into Clear, Windy, Rainy, or Snowy, based on deviations from mean values of precipitation, snow, and wind speed\n",
    "\n",
    "- **Time of Day Analysis**: \n",
    "\n",
    "    We categorize hours into Morning, Afternoon, Evening, and Night to reflect daily traffic patterns\n",
    "\n",
    "- **Rush Hour Identification**: \n",
    "\n",
    "    We flag rush hours, considering typical peak traffic times in the mornings and evenings\n",
    "\n",
    "- **Lockdown in France**:\n",
    "\n",
    "    Since our data reflects the period during the Covid-19 pandemic, it could be interesting to look at the lockdown dates in France, since bike trends should change during those times. We will use the package `lockdowndates` and merge it with our original data set.\n",
    "\n",
    "These new features are designed to deepen our understanding of the factors influencing bike traffic, thereby enhancing the predictive power of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'weather' based on conditions\n",
    "def categorize_weather(row, mean_rain, mean_snow, mean_windspeed):\n",
    "\n",
    "    #Goal: Create a column weather with the categories: Clear (could also be cloudy), Windy, Rain, Snow\n",
    "    #Consider the above if the value is greater than the mean value.\n",
    "    if row['prcp'] > mean_rain:\n",
    "        return 'Rainy'\n",
    "    elif row['snow'] > mean_snow:\n",
    "        return 'Snowy'\n",
    "    elif row['wspd'] > mean_windspeed:\n",
    "        return 'Windy'\n",
    "    else:\n",
    "        return 'Clear' #Note, we cannot differentiate whether it is sunny or cloudy. Just that the conditions above are not met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time of Day Category\n",
    "def categorize_time_of_day(hour):\n",
    "        if 6 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 18:\n",
    "            return 'Afternoon'\n",
    "        elif 18 <= hour < 24:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_rush_hour(hour):\n",
    "    \"\"\"\n",
    "    Flags rush hour periods based on the hour of the day.\n",
    "\n",
    "    Args:\n",
    "    hour (int): Hour of the day (0-23).\n",
    "\n",
    "    Returns:\n",
    "    int: 1 if it's rush hour, otherwise 0.\n",
    "    \"\"\"\n",
    "    # Define morning and evening rush hours (you can adjust these based on local patterns)\n",
    "    morning_rush = (7, 8, 9)\n",
    "    evening_rush = (16, 17, 18)\n",
    "\n",
    "    if hour in morning_rush or hour in evening_rush:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lockdown_curfew_features(df):\n",
    "    \"\"\"\n",
    "    Add lockdown and curfew features to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with lockdown and curfew features added.\n",
    "    \"\"\"\n",
    "    # Reset the index to make 'date' a column if it's not already a column\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "    # Get the start and end dates in string format\n",
    "    start_date_str = df['date'].min().strftime('%Y-%m-%d')\n",
    "    end_date_str = df['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Initialize LockdownDates for France with the specified dates and restrictions\n",
    "    ld = LockdownDates(\"France\", start_date_str, end_date_str, (\"stay_at_home\", \"masks\"))\n",
    "    lockdown_dates = ld.dates()\n",
    "\n",
    "    # Check if the returned DataFrame from LockdownDates is empty\n",
    "    if not lockdown_dates.empty:\n",
    "        # Merge the lockdown information based on the date\n",
    "        df = df.merge(lockdown_dates, left_on='date', right_index=True, how='left')\n",
    "\n",
    "        df.drop(columns='france_country_code', inplace=True)\n",
    "\n",
    "        # Fill NaN values that resulted from merge operation\n",
    "        df['france_masks'].fillna(0, inplace=True)\n",
    "        df['france_stay_at_home'].fillna(0, inplace=True)\n",
    "\n",
    "        df['france_masks'] = df['france_masks'].astype(int)\n",
    "        df['france_stay_at_home'] = df['france_stay_at_home'].astype(int)\n",
    "\n",
    "        # Rename columns\n",
    "        df.rename(columns={'france_masks': 'masks_code', 'france_stay_at_home': 'stay_at_home_code'}, inplace=True)\n",
    "\n",
    "    else:\n",
    "        # If no lockdown data is available, add default columns with 0\n",
    "        df['france_masks'] = 0\n",
    "        df['france_stay_at_home'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(df,mean_rain, mean_snow, mean_windspeed):\n",
    "    \"\"\"\n",
    "    Create new features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with new features.\n",
    "    \"\"\"\n",
    "    # Categorizing weather\n",
    "    df['weather'] = df.apply(lambda row: categorize_weather(row, mean_rain, mean_snow, mean_windspeed), axis=1)\n",
    "\n",
    "    # Categorizing time of the day\n",
    "    df['time_of_day'] = df['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "    # Flagging rush hour\n",
    "    df['rush_hour'] = df['hour'].apply(flag_rush_hour)\n",
    "    \n",
    "    # Seasonal Trends\n",
    "    # TO BE DONE ? NYE, tour de france, events/nuits blanches/festivals etc.\n",
    "    #df['high_season'] = df['month'].apply(lambda x: 1 if x in [6, 7, 8] else 0)  # Summer months as high season\n",
    "\n",
    "    # Historical Traffic Patterns\n",
    "    # This feature requires historical data\n",
    "    # Consider previous bike data in paris ? require external past data\n",
    "    # df['historical_avg'] = calculate_historical_avg(df)\n",
    "\n",
    "    #other ideas:\n",
    "    #Public Transport Strikes\n",
    "    #Daylight Hours\n",
    "    #COVID-19 Restrictions\n",
    "\n",
    "    df = add_lockdown_curfew_features(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation\n",
    "In the feature transformation phase of our project, we focus on optimizing and tailoring our dataset for predictive modeling:\n",
    "\n",
    "- **Dataset Segmentation**: \n",
    "\n",
    "    We introduce an innovative step to split the dataset based on the `working_day` feature. This allows us to separately analyze and model bike traffic patterns for working days and non-working days, recognizing that these two groups may exhibit distinct behaviors.\n",
    "\n",
    "**Note:** We will apply `one-hot encoding` to categorical variables like `site_name`, `weather`, `month`, and `hour` when creating our pipeline for the machine learning model. \n",
    "\n",
    "These transformations are key to ensuring our model accurately captures the nuances of bike traffic patterns in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_working_day(df):\n",
    "    \"\"\"\n",
    "    Splits the dataset into two based on working day.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    tuple: DataFrames split by working day.\n",
    "    \"\"\"\n",
    "    # Splitting dataset\n",
    "    df_working_day = df[df['working_day'] == 1]\n",
    "    df_non_working_day = df[df['working_day'] == 0]\n",
    "\n",
    "    df_working_day.drop(columns=['working_day'], inplace=True)\n",
    "    df_non_working_day.drop(columns=['working_day'], inplace=True)\n",
    "    \n",
    "    return df_working_day, df_non_working_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prep and Conclusion\n",
    "\n",
    "In the final part of our project, we bring together all feature engineering steps to prepare our dataset for model training and evaluation:\n",
    "\n",
    "#### 1. Encoding Dates and Weather Data: \n",
    "\n",
    "We start by encoding date-related features and incorporating weather data into our dataset.\n",
    "\n",
    "#### 2. Data Cleaning: \n",
    "\n",
    "Next, we clean the dataset, handling missing values and removing duplicates.\n",
    "\n",
    "#### 3. Feature Creation and Transformation: \n",
    "\n",
    "We create new features, including weather categorization, and apply transformations like one-hot encoding.\n",
    "\n",
    "#### 4. Data Splitting and Saving: \n",
    "\n",
    "Finally, we split the dataset based on working days and save the processed data for both training and testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_index(df):\n",
    "    \"\"\"\n",
    "    Sets the 'date' column as the index of the DataFrame, if it exists.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with 'time' set as the index.\n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])  # Ensure 'date' is in datetime format\n",
    "        df.set_index('date', inplace=True)\n",
    "        print(\"Date column set as index.\")\n",
    "    else:\n",
    "        print(\"Date column not found in DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, knn_flag=False, save_path=None, load_path=None):\n",
    "    \"\"\"\n",
    "    Handles missing values in the DataFrame with specific strategies for each column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    knn_flag (bool): Flag to determine whether to perform KNN imputation.\n",
    "    save_path (str): Path to save the DataFrame after KNN imputation.\n",
    "    load_path (str): Path to load the DataFrame if KNN imputation is not performed.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame after handling missing values.\n",
    "    \"\"\"\n",
    "    # Define columns that require specific imputation strategies\n",
    "    zero_fill_cols = ['prcp']  # Assuming no precipitation for missing values\n",
    "\n",
    "    # Fill with zeros\n",
    "    for col in zero_fill_cols:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "\n",
    "    # KNN Imputation for 'snow'\n",
    "    if load_path == 'data/bike_df_imputed.parquet':\n",
    "        # Load the DataFrame with the imputed values from a parquet file\n",
    "        df['snow'] = pd.read_parquet(load_path)['snow']\n",
    "\n",
    "    elif 'snow' in df.columns and knn_flag:\n",
    "        # Perform KNN imputation\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        df[['snow']] = knn_imputer.fit_transform(df[['snow']].to_numpy().reshape(-1, 1))\n",
    "\n",
    "        # Save the DataFrame with the imputed values, only if the file doesn't already exist\n",
    "        if save_path and not os.path.exists(save_path):\n",
    "            df.to_parquet(save_path)\n",
    "\n",
    "    # Check and fill any remaining missing values for other columns\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            # Choose a default imputation strategy for other columns (e.g., median)\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a DataFrame and provide information about the removal.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing potentially duplicate rows.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame with duplicate rows removed.\n",
    "    \"\"\"\n",
    "    initial_row_count = df.shape[0]\n",
    "    df = df.drop_duplicates()\n",
    "    final_row_count = df.shape[0]\n",
    "\n",
    "    print(f\"Removed {initial_row_count - final_row_count} duplicates. Remaining rows: {final_row_count}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, visualization=False):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by handling missing values, removing duplicates, setting the time index, \n",
    "    and dropping redundant or low correlation columns.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Handling missing values\n",
    "    # If performing KNN imputation and saving the result (otherwise, set knn_flag = False)\n",
    "    df = handle_missing_values(df, knn_flag=True, save_path='data/bike_df_imputed.parquet', load_path='data/bike_df_imputed.parquet')\n",
    "\n",
    "    # Removing duplicates\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    # Setting the time index\n",
    "    df = set_time_index(df)\n",
    "\n",
    "    # List of columns to drop\n",
    "\n",
    "    if visualization == True:\n",
    "        # We are keeping more features for visualization purposes, even if in our prediction model, we will not use them\n",
    "        drop_columns = [\n",
    "            'site_name', 'site_id', 'coordinates', 'latitude', 'longitude', 'tsun', \n",
    "            'counter_id', 'counter_installation_date', 'counter_technical_id', 'date'\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        drop_columns = [\n",
    "            'counter_name', 'site_id', 'coordinates', 'bike_count', 'latitude', 'longitude', 'year', \n",
    "            'day', 'tsun', 'weekday', 'coco', 'pres', 'wspd', 'wdir', 'dwpt', 'date',\n",
    "            'counter_id', 'counter_installation_date', 'counter_technical_id'\n",
    "        ]\n",
    "    \n",
    "    # Only drop columns that are present in the DataFrame\n",
    "    columns_to_drop = [col for col in drop_columns if col in df.columns]\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, visualization=False):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering steps to the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame after feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply date encoding\n",
    "    df = encode_dates(df)\n",
    "    \n",
    "    # Incorporate weather data\n",
    "    df = get_weather_data(df)\n",
    "\n",
    "    # Calculate mean values for each weather column before cleaning\n",
    "    mean_rain = df['prcp'].mean()\n",
    "    mean_snow = df['snow'].mean()\n",
    "    mean_windspeed = df['wspd'].mean()\n",
    "\n",
    "    # Create new features (including weather category using the calculated means)\n",
    "    df = create_new_features(df, mean_rain, mean_snow, mean_windspeed)\n",
    "\n",
    "    # Clean the data (handle missing values, remove duplicates, etc.)\n",
    "    df = clean_data(df, visualization)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching lockdown dates...\n",
      "Fetched lockdown dates for: France\n",
      "Removed 0 duplicates. Remaining rows: 496827\n",
      "Date column set as index.\n",
      "Fetching lockdown dates...\n",
      "Fetched lockdown dates for: France\n",
      "Removed 0 duplicates. Remaining rows: 41608\n",
      "Date column set as index.\n",
      "Fetching lockdown dates...\n",
      "Fetched lockdown dates for: France\n",
      "Removed 0 duplicates. Remaining rows: 496827\n",
      "Date column set as index.\n",
      "Fetching lockdown dates...\n",
      "Fetched lockdown dates for: France\n",
      "Removed 0 duplicates. Remaining rows: 41608\n",
      "Date column set as index.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "bike_df_train = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "bike_df_test = pd.read_parquet(Path(\"data\") / \"test.parquet\")\n",
    "\n",
    "train_processed_viz = feature_engineering(bike_df_train, visualization=True)\n",
    "test_processed_viz = feature_engineering(bike_df_test, visualization=True)\n",
    "\n",
    "train_processed = feature_engineering(bike_df_train)\n",
    "test_processed = feature_engineering(bike_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_datasets(train_df, test_df, save_train_path=None, save_test_path=None):\n",
    "    # Combine columns from both datasets\n",
    "    all_columns = set(train_df.columns).union(set(test_df.columns))\n",
    "\n",
    "    # Reindex both datasets to have the same columns, fill missing with 0\n",
    "    train_df_aligned = train_df.reindex(columns=all_columns, fill_value=0)\n",
    "    test_df_aligned = test_df.reindex(columns=all_columns, fill_value=0)\n",
    "\n",
    "    # Optionally save the aligned datasets\n",
    "    if save_train_path:\n",
    "        train_df_aligned.to_parquet(save_train_path)\n",
    "    if save_test_path:\n",
    "        test_df_aligned.to_parquet(save_test_path)\n",
    "\n",
    "    return train_df_aligned, test_df_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_aligned_processed_data(train_df, test_df, filename, split_by_working_day=True):\n",
    "    # Align datasets\n",
    "    train_aligned, test_aligned = align_datasets(train_df, test_df)\n",
    "\n",
    "    # Process for working and non-working day datasets\n",
    "    if split_by_working_day:\n",
    "        for df, part in [(train_aligned, \"train\"), (test_aligned, \"test\")]:\n",
    "            df_working_day, df_non_working_day = split_dataset_by_working_day(df)\n",
    "            df_working_day.to_parquet(Path(\"data\") / f\"{part}_{filename}_working_day.parquet\")\n",
    "            df_non_working_day.to_parquet(Path(\"data\") / f\"{part}_{filename}_non_working_day.parquet\")\n",
    "    else:\n",
    "        # Save aligned datasets without splitting\n",
    "        train_aligned.to_parquet(Path(\"data\") / f\"train_{filename}.parquet\")\n",
    "        test_aligned.to_parquet(Path(\"data\") / f\"test_{filename}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Don't split for visualization\n",
    "save_aligned_processed_data(train_processed_viz, test_processed_viz, \"processed_viz\", split_by_working_day=False)\n",
    "\n",
    "# Split for modeling\n",
    "save_aligned_processed_data(train_processed, test_processed, \"processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach and Challenges:\n",
    "\n",
    "We anticipate challenges in ensuring feature relevance and avoiding overfitting. We will tackle these through careful feature selection and validation, ensuring our model remains generalizable and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "This notebook is critical for setting up a data foundation that’s not only rich in information but also tailored for effective modeling in the next stages. The quality and ingenuity of the features engineered here will be a determinant factor in the accuracy of our predictions.\n",
    "\n",
    "Now that we added a lot of new features, we can go back into the EDA stage (`part 3 - post processing visualization`) to justify our approach on feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bikes-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
