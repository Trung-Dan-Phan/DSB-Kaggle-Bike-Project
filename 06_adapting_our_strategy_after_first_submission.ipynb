{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Adapting Our Strategy After First Submission\n",
    "\n",
    "Our first Kaggle submission was a reality check. We landed at the bottom of the leaderboard, signaling that our approach needed a serious revision. This prompted us to shift gears from building a complex model to a more iterative, cautious approach.\n",
    "\n",
    "We realized the necessity of starting with a simple model, one that was robust against overfitting, and then incrementally introducing complexity. This method allowed us to evaluate each feature's impact thoroughly, ensuring we were capturing valuable insights rather than just data noise.\n",
    "\n",
    "In this file, we document how we deconstructed our initial complex model and rebuilt it, step by step. We focused on discerning which features genuinely mattered, refining our feature engineering process to enhance model performance.\n",
    "\n",
    "This journey from the last rank to a top-tier position underscores our learning curve and adaptability. It highlights how strategic changes, grounded in thoughtful analysis and careful experimentation, can lead to significant improvements in a competitive environment like Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Kaggle Submission\n",
    "\n",
    "$$ Score : 2.1887 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install('pyarrow')\n",
    "install('vacances-scolaires-france')\n",
    "install('meteostat')\n",
    "install('lockdowndates')\n",
    "\n",
    "#imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from sklearn.impute import SimpleImputer\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "import os\n",
    "import holidays\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from lockdowndates.core import LockdownDates\n",
    "import catboost as cb\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "#Load the data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "\n",
    "# Feature engineering part\n",
    "\n",
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'Spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'Summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter' # December, January, February\n",
    "\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "# Initialize the SchoolHolidayDates object\n",
    "school_holidays = SchoolHolidayDates()\n",
    "\n",
    "#Check if a given date is a school holiday\n",
    "def is_school_holiday(datetime_obj):\n",
    "    # Extracting just the date part from the datetime object\n",
    "    date_obj = datetime_obj.date()\n",
    "    return school_holidays.is_holiday_for_zone(date_obj, 'C')  # Paris is Zone C\n",
    "\n",
    "def encode_dates(df):\n",
    "    if 'date' not in df.columns or not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        raise ValueError(\"DataFrame must have a 'date' column of datetime type.\")\n",
    "\n",
    "    X = df.copy()\n",
    "\n",
    "    # Extracting date components\n",
    "    X['year'] = X['date'].dt.year\n",
    "    X['month'] = X['date'].dt.month\n",
    "    X['day'] = X['date'].dt.day\n",
    "    X['hour'] = X['date'].dt.hour\n",
    "    X['weekday'] = X['date'].dt.weekday\n",
    "\n",
    "    # Adding season based on month\n",
    "    X['season'] = X['month'].apply(get_season)\n",
    "\n",
    "    # Identifying working days (weekdays not in French holidays)\n",
    "    fr_holidays = holidays.France()\n",
    "    X['working_day'] = ((X['date'].dt.weekday < 5) & ~X['date'].dt.date.isin(fr_holidays)).astype(int)\n",
    "\n",
    "    # Identifying school holidays\n",
    "    X['school_holiday'] = X['date'].apply(is_school_holiday).astype(int)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_weather_data(X):\n",
    "    if not all(col in X.columns for col in ['date', 'latitude', 'longitude']):\n",
    "        raise ValueError(\"DataFrame must have 'date', 'latitude', and 'longitude' columns.\")\n",
    "\n",
    "    unique_locations = X[['latitude', 'longitude']].drop_duplicates()\n",
    "    weather_data_list = []\n",
    "\n",
    "    for lat, lon in unique_locations.itertuples(index=False):\n",
    "        point = Point(lat, lon)\n",
    "        location_data = Hourly(point, X['date'].min(), X['date'].max()).fetch()\n",
    "        location_data.reset_index(inplace=True)  # Reset index to make 'date' a column\n",
    "        location_data.rename(columns={'time': 'date'}, inplace=True)  # Rename 'time' column to 'date'\n",
    "        location_data['latitude'], location_data['longitude'] = lat, lon\n",
    "        weather_data_list.append(location_data)\n",
    "\n",
    "    weather_data = pd.concat(weather_data_list, ignore_index=True)\n",
    "\n",
    "    # Ensure 'date' column is in the right format if it's not already\n",
    "    if weather_data['date'].dtype != 'datetime64[ns]':\n",
    "        weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "\n",
    "    # Merge data\n",
    "    merged_data = pd.merge(X, weather_data, on=['date', 'latitude', 'longitude'], how='left')\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def handle_missing_values(df, knn_flag=False, save_path=None, load_path=None):\n",
    "    \"\"\"\n",
    "    Handles missing values in the DataFrame with specific strategies for each column.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    knn_flag (bool): Flag to determine whether to perform KNN imputation.\n",
    "    save_path (str): Path to save the DataFrame after KNN imputation.\n",
    "    load_path (str): Path to load the DataFrame if KNN imputation is not performed.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame after handling missing values.\n",
    "    \"\"\"\n",
    "    # Define columns that require specific imputation strategies\n",
    "    zero_fill_cols = ['prcp', 'snow']  # Assuming no precipitation for missing values\n",
    "    mean_fill_cols = ['temp', 'rhum', 'wspd']  # Using mean for these columns\n",
    "    #knn_fill_cols = ['snow']  # KNN imputation for snow\n",
    "\n",
    "    # Fill with zeros\n",
    "    for col in zero_fill_cols:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "\n",
    "    # Mean imputation\n",
    "    mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    for col in mean_fill_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = mean_imputer.fit_transform(df[[col]])\n",
    "\n",
    "\n",
    "    # Check and fill any remaining missing values for other columns\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            # Choose a default imputation strategy for other columns (e.g., median)\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by handling missing values, removing duplicates, setting the time index, \n",
    "    and dropping redundant or low correlation columns.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Handling missing values\n",
    "    # If performing KNN imputation and saving the result\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # List of columns to drop\n",
    "    drop_columns = ['site_name', 'site_id', 'coordinates', 'bike_count', 'latitude', 'longitude', 'year', 'day', 'tsun', 'weekday', \n",
    "                    'counter_id', 'counter_installation_date', 'counter_technical_id', 'date'\n",
    "                     #'season'\n",
    "                     ]\n",
    "    \n",
    "    # Only drop columns that are present in the DataFrame\n",
    "    columns_to_drop = [col for col in drop_columns if col in df.columns]\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create a new column 'weather' based on conditions\n",
    "def categorize_weather(row, mean_rain, mean_snow, mean_windspeed):\n",
    "\n",
    "    #Goal: Create a column weather with the categories: Clear (could also be cloudy), Windy, Rain, Snow\n",
    "    #Consider the above if the value is greater than the mean value.\n",
    "    if row['prcp'] > mean_rain:\n",
    "        return 'Rainy'\n",
    "    elif row['snow'] > mean_snow:\n",
    "        return 'Snowy'\n",
    "    elif row['wspd'] > mean_windspeed:\n",
    "        return 'Windy'\n",
    "    else:\n",
    "        return 'Clear' #Note, we cannot differentiate whether it is sunny or cloudy. Just that the conditions above are not met\n",
    "\n",
    "# Time of Day Category\n",
    "def categorize_time_of_day(hour):\n",
    "        if 6 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 18:\n",
    "            return 'Afternoon'\n",
    "        elif 18 <= hour < 24:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "\n",
    "def flag_rush_hour(hour):\n",
    "    \"\"\"\n",
    "    Flags rush hour periods based on the hour of the day.\n",
    "\n",
    "    Args:\n",
    "    hour (int): Hour of the day (0-23).\n",
    "\n",
    "    Returns:\n",
    "    int: 1 if it's rush hour, otherwise 0.\n",
    "    \"\"\"\n",
    "    # Define morning and evening rush hours (you can adjust these based on local patterns)\n",
    "    morning_rush = (7, 8, 9)\n",
    "    evening_rush = (16, 17, 18)\n",
    "\n",
    "    if hour in morning_rush or hour in evening_rush:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def add_lockdown_curfew_features(df):\n",
    "    \"\"\"\n",
    "    Add lockdown and curfew features to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with lockdown and curfew features added.\n",
    "    \"\"\"\n",
    "    # Reset the index to make 'date' a column if it's not already a column\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "    # Get the start and end dates in string format\n",
    "    start_date_str = df['date'].min().strftime('%Y-%m-%d')\n",
    "    end_date_str = df['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Initialize LockdownDates for France with the specified dates and restrictions\n",
    "    ld = LockdownDates(\"France\", start_date_str, end_date_str, (\"stay_at_home\", \"masks\"))\n",
    "    lockdown_dates = ld.dates()\n",
    "\n",
    "    # Check if the returned DataFrame from LockdownDates is empty\n",
    "    if not lockdown_dates.empty:\n",
    "        # Merge the lockdown information based on the date\n",
    "        df = df.merge(lockdown_dates, left_on='date', right_index=True, how='left')\n",
    "\n",
    "        df.drop(columns='france_country_code', inplace=True)\n",
    "\n",
    "        # Fill NaN values that resulted from merge operation\n",
    "        df['france_masks'].fillna(0, inplace=True)\n",
    "        df['france_stay_at_home'].fillna(0, inplace=True)\n",
    "\n",
    "        df['france_masks'] = df['france_masks'].astype(int)\n",
    "        df['france_stay_at_home'] = df['france_stay_at_home'].astype(int)\n",
    "\n",
    "        # Rename columns\n",
    "        df.rename(columns={'france_masks': 'masks_code', 'france_stay_at_home': 'stay_at_home_code'}, inplace=True)\n",
    "\n",
    "    else:\n",
    "        # If no lockdown data is available, add default columns with 0\n",
    "        df['france_masks'] = 0\n",
    "        df['france_stay_at_home'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_new_features(df,mean_rain, mean_snow, mean_windspeed):\n",
    "    \"\"\"\n",
    "    Create new features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with new features.\n",
    "    \"\"\"\n",
    "    # Categorizing weather\n",
    "    df['weather'] = df.apply(lambda row: categorize_weather(row, mean_rain, mean_snow, mean_windspeed), axis=1)\n",
    "\n",
    "    # Categorizing time of the day\n",
    "    df['time_of_day'] = df['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "    # Flagging rush hour\n",
    "    df['rush_hour'] = df['hour'].apply(flag_rush_hour)\n",
    "    \n",
    "    df = add_lockdown_curfew_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_transformation(df):\n",
    "    \"\"\"\n",
    "    Transforms features in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    # Dropping redundant or low correlation columns\n",
    "    #drop_columns = ['counter_name', 'bike_count', 'latitude', 'longitude', 'year', 'day', 'weekday', 'season']\n",
    "    #df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    # One-hot encoding for categorical variables\n",
    "    df = pd.get_dummies(df, columns=['counter_name', 'weather', 'month', 'hour', 'season', 'time_of_day'], drop_first=True,  dtype=int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_dataset_by_working_day(df):\n",
    "    \"\"\"\n",
    "    Splits the dataset into two based on working day.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    tuple: DataFrames split by working day.\n",
    "    \"\"\"\n",
    "    # Splitting dataset\n",
    "    df_working_day = df[df['working_day'] == 1]\n",
    "    df_non_working_day = df[df['working_day'] == 0]\n",
    "\n",
    "    return df_working_day, df_non_working_day\n",
    "\n",
    "def align_datasets(train_df, test_df):\n",
    "    # Combine columns from both datasets\n",
    "    all_columns = set(train_df.columns).union(set(test_df.columns))\n",
    "\n",
    "    # Reindex both datasets to have the same columns, fill missing with 0\n",
    "    train_df_aligned = train_df.reindex(columns=all_columns, fill_value=0)\n",
    "    test_df_aligned = test_df.reindex(columns=all_columns, fill_value=0)\n",
    "\n",
    "    return train_df_aligned, test_df_aligned\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering steps to the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame after feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply date encoding\n",
    "    df = encode_dates(df)\n",
    "    \n",
    "    # Incorporate weather data\n",
    "    df = get_weather_data(df)\n",
    "\n",
    "    # Calculate mean values for each weather column before cleaning\n",
    "    mean_rain = df['prcp'].mean()\n",
    "    mean_snow = df['snow'].mean()\n",
    "    mean_windspeed = df['wspd'].mean()\n",
    "\n",
    "    # Create new features (including weather category using the calculated means)\n",
    "    df = create_new_features(df, mean_rain, mean_snow, mean_windspeed)\n",
    "\n",
    "    # Transform features (one-hot encoding, etc.)\n",
    "    df = feature_transformation(df)\n",
    "    \n",
    "    # Clean the data (handle missing values, etc.)\n",
    "    df = clean_data(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "train_processed = feature_engineering(bike_df_train)\n",
    "test_processed = feature_engineering(bike_df_test)\n",
    "\n",
    "# Split between working day and non working day\n",
    "train_processed_w, train_processed_nw = split_dataset_by_working_day(train_processed)\n",
    "test_processed_w, test_processed_nw = split_dataset_by_working_day(test_processed)\n",
    "\n",
    "\n",
    "# modeling part\n",
    "\n",
    "# Separate features and target variables for training\n",
    "# For working days\n",
    "X_train_w = train_processed_w.drop(columns=[\"log_bike_count\"])\n",
    "y_train_w = train_processed_w[\"log_bike_count\"]\n",
    "# For non-working days\n",
    "X_train_nw = train_processed_nw.drop(columns=[\"log_bike_count\"])\n",
    "y_train_nw = train_processed_nw[\"log_bike_count\"]\n",
    "\n",
    "# Reindex both datasets to have the same columns, fill missing with 0\n",
    "X_train_w, X_test_w = align_datasets(X_train_w, test_processed_w)\n",
    "X_train_nw, X_test_nw = align_datasets(X_train_nw, test_processed_nw)\n",
    "\n",
    "# catboost optuna tuned study results\n",
    "params_w = {\n",
    "    'iterations': 1096, \n",
    "    'depth': 9, \n",
    "    'learning_rate': 0.22603548878280666, \n",
    "    'random_strength': 2, \n",
    "    'bagging_temperature': 0.1867337573932248, \n",
    "    'l2_leaf_reg': 2.304593084966779e-05, \n",
    "    'border_count': 66, \n",
    "    'grow_policy': 'Lossguide',\n",
    "    'loss_function': 'RMSE',\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "params_nw = {\n",
    "    'iterations': 366,\n",
    "    'depth': 9,\n",
    "    'learning_rate': 0.13516379949083754,\n",
    "    'random_strength': 10,\n",
    "    'bagging_temperature': 0.27795134630855506,\n",
    "    'l2_leaf_reg': 0.10661335848192686,\n",
    "    'border_count': 1,\n",
    "    'loss_function': 'RMSE',\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Create and train the CatBoost model for working days\n",
    "model_w = cb.CatBoostRegressor(**params_w)\n",
    "model_w.fit(X_train_w, y_train_w, verbose=False)\n",
    "\n",
    "# Predict for working days\n",
    "y_pred_w = model_w.predict(X_test_w)\n",
    "\n",
    "# Create and train the CatBoost model for non-working days\n",
    "model_nw = cb.CatBoostRegressor(**params_nw)\n",
    "model_nw.fit(X_train_nw, y_train_nw, verbose=False)\n",
    "\n",
    "# Predict for non-working days\n",
    "y_pred_nw = model_nw.predict(X_test_nw)\n",
    "\n",
    "# Create dataframes with predictions and test data\n",
    "df_pred_w = pd.DataFrame({'y_pred_w': y_pred_w}, index=X_test_w.index)\n",
    "df_pred_nw = pd.DataFrame({'y_pred_nw': y_pred_nw}, index=X_test_nw.index)\n",
    "\n",
    "#merge based on date index\n",
    "# Concatenate the dataframes vertically\n",
    "y_pred = pd.concat([df_pred_w, df_pred_nw])\n",
    "\n",
    "# Rename the index to make it consistent (optional)\n",
    "y_pred.index.name = 'index'\n",
    "\n",
    "# Add a common column name 'predictions'\n",
    "y_pred['predictions'] = y_pred['y_pred_w'].combine_first(y_pred['y_pred_nw'])\n",
    "\n",
    "# Drop the individual prediction columns if needed\n",
    "y_pred = y_pred.drop(['y_pred_w', 'y_pred_nw'], axis=1)\n",
    "\n",
    "# Create dataframe in the right format for Kaggle submission\n",
    "results = pd.DataFrame(\n",
    "    dict(\n",
    "        Id=np.arange(y_pred.shape[0]),\n",
    "        log_bike_count=y_pred['predictions'].tolist(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to CSV for submission\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first Kaggle attempt, scoring 2.1887, revealed severe overfitting due to an overly complex model. So we pivoted to a simpler, iterative approach, carefully adding and evaluating features. This strategic shift led to a significant improvement in our leaderboard position, demonstrating the effectiveness of a balanced and adaptable model-building strategy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the Model: A Key Breakthrough\n",
    "\n",
    "$$ Previous Best Score : 2.1887 $$\n",
    "$$ New Best Score : 0.6722 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "#install('vacances-scolaires-france')\n",
    "#install('meteostat')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Encode dates function (simplified to essential components)\n",
    "def encode_dates(df):\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    return df.drop(columns=['date'])\n",
    "\n",
    "# Apply date encoding to the datasets\n",
    "bike_df_train = encode_dates(bike_df_train)\n",
    "bike_df_test = encode_dates(bike_df_test)\n",
    "\n",
    "# Column selection (simplified)\n",
    "columns_to_use = ['year', 'month', 'day', 'hour', 'weekday', 'counter_name']\n",
    "\n",
    "# Preprocessing pipeline (simplified)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"std_scaler\", StandardScaler(), ['year', 'month', 'day', 'hour', 'weekday']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"counter_name\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# XGBoost regressor (simplified)\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Full pipeline\n",
    "pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Model Complexity: CatBoost Implementation\n",
    "shift to a different model (CatBoost) while still focusing on simplicity\n",
    "\n",
    "$$ Previous Best Score : 0.6722 $$\n",
    "$$ New Best Score : 0.6518$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler version using catboost\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "#install('vacances-scolaires-france')\n",
    "#install('meteostat')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Encode dates function (simplified to essential components)\n",
    "def encode_dates(df):\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    return df.drop(columns=['date'])\n",
    "\n",
    "# Apply date encoding to the datasets\n",
    "bike_df_train = encode_dates(bike_df_train)\n",
    "bike_df_test = encode_dates(bike_df_test)\n",
    "\n",
    "# Column selection (simplified)\n",
    "columns_to_use = ['year', 'month', 'day', 'hour', 'weekday', 'counter_name']\n",
    "\n",
    "# Preprocessing pipeline (simplified)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"std_scaler\", StandardScaler(), ['year', 'month', 'day', 'hour', 'weekday']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"counter_name\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# CatBoost regressor (simplified)\n",
    "model = CatBoostRegressor(verbose=0)  # Set verbose to 0 to reduce log output\n",
    "\n",
    "# Full pipeline\n",
    "pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the Model: Simple GridSearch Tuning\n",
    "At this stage, we decided to check how a simple tuning strategy would modify our score before adding more complex features and risking to overfit\n",
    "\n",
    "$$ Previous Best Score : 0.6518 $$\n",
    "$$ New Best Score : 0.6358 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler version using catboost\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "#install('vacances-scolaires-france')\n",
    "#install('meteostat')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Encode dates function (simplified to essential components)\n",
    "def encode_dates(df):\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    return df.drop(columns=['date'])\n",
    "\n",
    "# Apply date encoding to the datasets\n",
    "bike_df_train = encode_dates(bike_df_train)\n",
    "bike_df_test = encode_dates(bike_df_test)\n",
    "\n",
    "# Column selection (simplified)\n",
    "columns_to_use = ['year', 'month', 'day', 'hour', 'weekday', 'counter_name']\n",
    "\n",
    "# Preprocessing pipeline (simplified)\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"std_scaler\", StandardScaler(), ['year', 'month', 'day', 'hour', 'weekday']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"counter_name\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_parms = {'iterations': 448, 'depth': 8, 'learning_rate': 0.19833451396224458, 'random_strength': 19, 'bagging_temperature': 0.7521783474435499, 'od_type': 'IncToDec', 'l2_leaf_reg': 0.2758522910440182}\n",
    "\n",
    "\n",
    "# CatBoost regressor (simplified)\n",
    "model = CatBoostRegressor(**best_parms, random_seed=42, verbose=0)  # Set verbose to 0 to reduce log output\n",
    "\n",
    "# Full pipeline\n",
    "pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Prepare submission\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating New Features: Balancing Simplicity and Complexity\n",
    "Careful addition of new features, balancing complexity with the risk of overfitting\n",
    "\n",
    "$$ Previous Best Score : 0.6358 $$\n",
    "$$ New Best Score : 0.6291 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Convert date to datetime and extract components\n",
    "bike_df_train['date'] = pd.to_datetime(bike_df_train['date'])\n",
    "bike_df_train['hour'] = bike_df_train['date'].dt.hour\n",
    "bike_df_train['day'] = bike_df_train['date'].dt.day\n",
    "bike_df_train['weekday'] = bike_df_train['date'].dt.weekday\n",
    "bike_df_train['month'] = bike_df_train['date'].dt.month\n",
    "bike_df_train['year'] = bike_df_train['date'].dt.year\n",
    "\n",
    "bike_df_test['date'] = pd.to_datetime(bike_df_test['date'])\n",
    "bike_df_test['hour'] = bike_df_test['date'].dt.hour\n",
    "bike_df_test['day'] = bike_df_test['date'].dt.day\n",
    "bike_df_test['weekday'] = bike_df_test['date'].dt.weekday\n",
    "bike_df_test['month'] = bike_df_test['date'].dt.month\n",
    "bike_df_test['year'] = bike_df_test['date'].dt.year\n",
    "\n",
    "# Feature Engineering\n",
    "# Time of day (morning, midday, afternoon, night)\n",
    "def categorize_hour(hour):\n",
    "    if 5 <= hour < 10:\n",
    "        return 'morning'\n",
    "    elif 10 <= hour < 15:\n",
    "        return 'midday'\n",
    "    elif 15 <= hour < 20:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "bike_df_train['time_of_day'] = bike_df_train['hour'].apply(categorize_hour)\n",
    "bike_df_test['time_of_day'] = bike_df_test['hour'].apply(categorize_hour)\n",
    "\n",
    "# Weekday/Weekend\n",
    "bike_df_train['is_weekend'] = bike_df_train['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "bike_df_test['is_weekend'] = bike_df_test['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Define columns to use\n",
    "columns_to_use = ['year', 'month', 'day', 'hour', 'weekday', 'latitude', 'longitude', \n",
    "                  'time_of_day', 'is_weekend', 'counter_name']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(), ['year', 'month', 'day', 'hour', 'weekday', 'latitude', 'longitude']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), ['time_of_day', 'counter_name']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "# Splitting the data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Create LGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=2000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Prepare submission\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolving the Model: Incorporating Weather and School Holidays data\n",
    "After getting a good score with simple features, and since tuning wasn't really improving our performance again, we decided it was time to add more complex features to better predict the bike traffic in paris. In particular we added the school holidays as well as the weather dimension.\n",
    "\n",
    "Since the data got bigger by adding external data and additional packages we decided to use the more efficient lightgbm which seemed the new best balance for our model.\n",
    "\n",
    "$$ Previous Best Score : 0.6291 $$\n",
    "$$ New Best Score : 0.6145 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install('vacances-scolaires-france')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Load the weather data\n",
    "weather_data = pd.read_csv(\"/kaggle/input/mdsb-2023/external_data.csv\")\n",
    "\n",
    "# Initialize the SchoolHolidayDates object\n",
    "school_holidays = SchoolHolidayDates()\n",
    "\n",
    "# Check if a given date is a school holiday\n",
    "def is_school_holiday(datetime_obj):\n",
    "    date_obj = datetime_obj.date()\n",
    "    return school_holidays.is_holiday_for_zone(date_obj, 'C')  # Paris is Zone C\n",
    "\n",
    "# Function to categorize weather\n",
    "def simplified_weather_categorization(row):\n",
    "    # Define your thresholds\n",
    "    temp_cold_threshold = 278.15\n",
    "    temp_warm_threshold = 298.15\n",
    "    rain_threshold = 1.0\n",
    "\n",
    "    temp = row['t']\n",
    "    rain = row.get('rr1', 0)\n",
    "\n",
    "    if rain >= rain_threshold:\n",
    "        return \"Rainy\"\n",
    "    elif temp <= temp_cold_threshold:\n",
    "        return \"Cold\"\n",
    "    elif temp >= temp_warm_threshold:\n",
    "        return \"Warm\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "# Apply the function to create the 'simplified_weather_category' column\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "weather_data['simplified_weather_category'] = weather_data.apply(simplified_weather_categorization, axis=1)\n",
    "\n",
    "# Merge with training and testing data\n",
    "bike_df_train = bike_df_train.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "bike_df_test = bike_df_test.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "\n",
    "# Convert date to datetime and extract components\n",
    "bike_df_train['date'] = pd.to_datetime(bike_df_train['date'])\n",
    "bike_df_test['date'] = pd.to_datetime(bike_df_test['date'])\n",
    "\n",
    "# Extracting date components and school holiday feature\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['is_school_holiday'] = df['date'].apply(is_school_holiday)\n",
    "\n",
    "# Feature Engineering\n",
    "def categorize_hour(hour):\n",
    "    if 5 <= hour < 10:\n",
    "        return 'morning'\n",
    "    elif 10 <= hour < 15:\n",
    "        return 'midday'\n",
    "    elif 15 <= hour < 20:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "bike_df_train['time_of_day'] = bike_df_train['hour'].apply(categorize_hour)\n",
    "bike_df_test['time_of_day'] = bike_df_test['hour'].apply(categorize_hour)\n",
    "\n",
    "bike_df_train['is_weekend'] = bike_df_train['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "bike_df_test['is_weekend'] = bike_df_test['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Define columns to use\n",
    "columns_to_use = ['year', 'month', 'day', 'hour', 'weekday', 'is_school_holiday', 'latitude', 'longitude', \n",
    "                  'time_of_day', 'is_weekend', 'counter_name', 'simplified_weather_category']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(), ['year', 'month', 'day', 'hour', 'weekday', 'latitude', 'longitude']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), ['time_of_day', 'is_school_holiday', 'counter_name', 'simplified_weather_category']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "# Splitting the data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Create LGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=2000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Prepare submission\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Enhancements: Optimizing Weather-Time Interactions\n",
    "Final tweaks to the model, focusing on specific interactions for improved accuracy\n",
    "\n",
    "$$ Previous Best Score : 0.6145 $$\n",
    "$$ New Best Score : 0.6044 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install('vacances-scolaires-france')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Load the weather data\n",
    "weather_data = pd.read_csv(\"/kaggle/input/mdsb-2023/external_data.csv\")\n",
    "\n",
    "# Initialize the SchoolHolidayDates object\n",
    "school_holidays = SchoolHolidayDates()\n",
    "\n",
    "def is_school_holiday(datetime_obj):\n",
    "    date_obj = datetime_obj.date()\n",
    "    return school_holidays.is_holiday_for_zone(date_obj, 'C')\n",
    "\n",
    "def simplified_weather_categorization(row):\n",
    "    temp_cold_threshold = 278.15\n",
    "    temp_warm_threshold = 298.15\n",
    "    rain_threshold = 1.0\n",
    "    temp = row['t']\n",
    "    rain = row.get('rr1', 0)\n",
    "    if rain >= rain_threshold:\n",
    "        return \"Rainy\"\n",
    "    elif temp <= temp_cold_threshold:\n",
    "        return \"Cold\"\n",
    "    elif temp >= temp_warm_threshold:\n",
    "        return \"Warm\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "weather_data['simplified_weather_category'] = weather_data.apply(simplified_weather_categorization, axis=1)\n",
    "\n",
    "bike_df_train = bike_df_train.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "bike_df_test = bike_df_test.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['is_school_holiday'] = df['date'].apply(is_school_holiday)\n",
    "\n",
    "def categorize_hour(hour):\n",
    "    if 5 <= hour < 10:\n",
    "        return 'morning'\n",
    "    elif 10 <= hour < 15:\n",
    "        return 'midday'\n",
    "    elif 15 <= hour < 20:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['time_of_day'] = df['hour'].apply(categorize_hour)\n",
    "\n",
    "def create_weather_time_interaction(row):\n",
    "    return f\"{row['simplified_weather_category']}_{row['time_of_day']}\"\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['weather_time_interaction'] = df.apply(create_weather_time_interaction, axis=1)\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "columns_to_use = ['year', 'month', 'hour', 'weekday', 'is_school_holiday', 'latitude', 'longitude', \n",
    "                  'time_of_day', 'is_weekend', 'counter_name', 'simplified_weather_category', 'weather_time_interaction']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(), ['year', 'month', 'hour', 'weekday', 'latitude', 'longitude']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), ['time_of_day', 'is_school_holiday', 'counter_name', 'simplified_weather_category', 'weather_time_interaction']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=2000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)\n",
    "\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Best Submission\n",
    "\n",
    "<p align=\"center\" style=\"color:#32CD32\">\n",
    "  <strong>Best Final Public Score : 0.6048</strong><br><br>\n",
    "  <strong>Best Final Private Score : 0.5900</strong>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install('vacances-scolaires-france')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "\n",
    "# Load data\n",
    "bike_df_train = pd.read_parquet(\"/kaggle/input/mdsb-2023/train.parquet\")\n",
    "bike_df_test = pd.read_parquet(\"/kaggle/input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "# Load the weather data\n",
    "weather_data = pd.read_csv(\"/kaggle/input/mdsb-2023/external_data.csv\")\n",
    "\n",
    "# Initialize the SchoolHolidayDates object\n",
    "school_holidays = SchoolHolidayDates()\n",
    "\n",
    "def is_school_holiday(datetime_obj):\n",
    "    date_obj = datetime_obj.date()\n",
    "    return school_holidays.is_holiday_for_zone(date_obj, 'C')\n",
    "\n",
    "def simplified_weather_categorization(row):\n",
    "    temp_cold_threshold = 278.15\n",
    "    temp_warm_threshold = 298.15\n",
    "    rain_threshold = 1.0\n",
    "    temp = row['t']\n",
    "    rain = row.get('rr1', 0)\n",
    "    if rain >= rain_threshold:\n",
    "        return \"Rainy\"\n",
    "    elif temp <= temp_cold_threshold:\n",
    "        return \"Cold\"\n",
    "    elif temp >= temp_warm_threshold:\n",
    "        return \"Warm\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "weather_data['date'] = pd.to_datetime(weather_data['date'])\n",
    "weather_data['simplified_weather_category'] = weather_data.apply(simplified_weather_categorization, axis=1)\n",
    "\n",
    "bike_df_train = bike_df_train.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "bike_df_test = bike_df_test.merge(weather_data[['date', 'simplified_weather_category']], on='date', how='left')\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['is_school_holiday'] = df['date'].apply(is_school_holiday)\n",
    "\n",
    "def categorize_hour(hour):\n",
    "    if 5 <= hour < 10:\n",
    "        return 'morning'\n",
    "    elif 10 <= hour < 15:\n",
    "        return 'midday'\n",
    "    elif 15 <= hour < 20:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['time_of_day'] = df['hour'].apply(categorize_hour)\n",
    "\n",
    "def create_weather_time_interaction(row):\n",
    "    return f\"{row['simplified_weather_category']}_{row['time_of_day']}\"\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['weather_time_interaction'] = df.apply(create_weather_time_interaction, axis=1)\n",
    "\n",
    "for df in [bike_df_train, bike_df_test]:\n",
    "    df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "columns_to_use = ['year', 'month', 'hour', 'weekday', 'is_school_holiday', 'latitude', \n",
    "                  'time_of_day', 'is_weekend', 'counter_name', 'simplified_weather_category', 'weather_time_interaction']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(), ['year', 'month', 'hour', 'weekday', 'latitude']),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), ['time_of_day', 'is_school_holiday', 'counter_name', 'simplified_weather_category', 'weather_time_interaction']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train = bike_df_train[columns_to_use]\n",
    "y_train = bike_df_train[\"log_bike_count\"]\n",
    "X_test = bike_df_test[columns_to_use]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_valid = preprocessor.transform(X_valid)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[train_data, valid_data],\n",
    "                  num_boost_round=2000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)\n",
    "\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "results = pd.DataFrame({'Id': np.arange(len(y_pred)), 'log_bike_count': y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : Reflections and Lessons Learned\n",
    "\n",
    "Our journey in this Kaggle challenge highlights the importance of adaptability and strategic thinking in data science. Initially hindered by overfitting due to a complex model, we shifted to a simpler, iterative approach, adding complexity only when beneficial. This change in strategy led to a steady climb up the leaderboard.\n",
    "\n",
    "Incorporating nuanced features like weather patterns and school holidays, we found a balance between feature richness and model simplicity, ultimately choosing LightGBM for its efficiency. Our final model, refined with careful feature selection, showcased our ability to blend data insights with technical proficiency.\n",
    "\n",
    "This experience reinforces key data science principles: start simple, evaluate rigorously, and adapt continuously. Our significant improvement in both public and private scores is a testament to these learnings. As we move forward, the lessons from this challenge will guide our future endeavors in data science, armed with a deeper understanding of model building and feature selection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
